{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `annadl_f19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Friday*, November 15, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Monday*, November 18, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Friday*, November 22, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.4**: Did the network finish training? Consider the generated text across epochs.\n",
    "1. In the early batches (0-10), the generated text looks very bad. Can you explain why the low diversity generated text contains almost only the symbol \" \" (that is, spaces)?\n",
    "2. The high diversity generated text is messed up too, but in a different way. Explain how.\n",
    "3. In later batches (20-30) what do you notice is off about the low diversity generated text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.6**: Do the same as above, but for 40 random letters (e.g. smash away on your keyboard) as seed. What happens? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.1**: In your own words, explain what the following function arguments do in\n",
    "the different model loading functions:\n",
    "1. `include_top`- Lets you decide to use pretrained model's final dense layers or not (true = use), if disabled, it essentially lets you use the model as a feature extractor \n",
    "1. `weights` - Determine which network to use the weights of\n",
    "1. `input_shape` - Optionally set which shape you want the input of the new network to be, useful if you want to do a different resolution that the network was natively trained on. Typicallly needs 3 channels\n",
    "1. `pooling` - Determine pooling mode for feature extraction if `include_top` is false. This adds a layer on top of the original model (minus the final output layer) and allows \n",
    "1. `classes` - Number of classes to classify image into, used when `include_top` is true and there is no input for `weights` which allows flexibility for which model is used.\n",
    "1. Explain what 'global pooling' does, and why it is needed when `include_top=False` - Global pooling acts like pooling on the size of the feature map. It works by pooling each \"channel\" within the feature map. Depending on the type of global pooling, it either uses the average or the max of the channels in the feature map. This ultimately saves a lot of perameters when `include_top=False`, and just generally prevents overfitting as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2**: Following Jason's example under 'Pre-Trained Model as Classifier'\n",
    "classify [this image](https://66.media.tumblr.com/tumblr_mc46e7Zm4R1qbqngeo1_1280.jpg).\n",
    "Print not just the most likely label, but everything that `decode_predictions` returns.\n",
    ">\n",
    "> ***Important***: *Don't use VGG as he does. It's 500 MB to download, and will take too long.\n",
    "> Use one of the smaller models instead ([here](https://keras.io/applications/#documentation-for-individual-models)'s an overview of model sizes), such as DenseNet121.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hammer (40.42%)\n",
      "hand_blower (29.07%)\n",
      "hatchet (17.35%)\n",
      "dumbbell (5.21%)\n",
      "power_drill (2.02%)\n"
     ]
    }
   ],
   "source": [
    "# example of using a pre-trained model as a classifier\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.applications.densenet import decode_predictions\n",
    "from keras.applications.densenet import DenseNet121\n",
    "# load an image from file\n",
    "image = load_img('/home/gringle/Downloads/data.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = DenseNet121()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "#label = label[0][0]\n",
    "# print the classification\n",
    "#print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "for item in label[0]:\n",
    "    print('%s (%.2f%%)' % (item[1], item[2]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1**: What is typically the input and output of an autoencoder? What loss function can be used?\n",
    ">\n",
    "> The input to an autoencoder is typically some high dimensional input that we want to reduce the dimensionality of while preserving important features. The loss function is calculated by taking the difference of the output to the original input (reconstruction loss). For varaitional autoencoders the loss function consists of the reconstruction loss minus KL-divergence function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.3**: Purely in terms of architecture, what is the difference between an autoencoder and a variational autoencoder (VAE)?\n",
    ">\n",
    "> The bottleneck of an variational autoencoder maps input to the mean and standard deviation of the inputs distribution. This can effectively compress the input into a lower dimension while preserving important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.4**: Regular autoencoders are trained to minimize a loss function with no regard to how the latent space is organized. Therefore, continuity is not guaranteed and similar datapoints may not be close to each other. We can thus say that the network is overfitting, because it uses any organization of training points in this space to minimize the loss, and is, therefore, not likely to work well with unseen data. VAEs are a regularized form of autoencoders, invented to solve this problem. Importantly, they guarantee that similar points are close in the latent space. How do they achieve this?\n",
    "    > * How are datapoints represented in the VAE latent space? What is the intuition behind this?\n",
    "    > * How is the loss function different? What is the purpose of the second term (the KL divergence)?\n",
    ">\n",
    "> *Hint: Check out this [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and read the section \"Intuitions about the regularisation\"*\n",
    ">\n",
    "> 1. There two properties the datapoints in the latent space: continuity makes it so two close points in the latent space should not give two completely different contents once decoded and completeness make it so that for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded. These properties tend to create a gradient over the encoded information in the latent space. This can help to \"fill in the gaps\" between two datapoints by generating data somewhere in between said two points.\n",
    ">\n",
    "> 2. The loss function consists of a reconstruction term and a regularization term (KL divergence). The reconstruction term measures the difference between the original input and the reconstruction. KL divergence measures the difference of the probability distributions which is usefull since variational autoencoders use distributions to encode data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.1**: Explain in your own words how the GAN works. Touch upon:\n",
    "    > * What do the generator and discriminator networks do?\n",
    "    > * What are their respective input and output?\n",
    "    > * What would the accuracy of the discriminator be, faced with a perfect generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.4**: How do you transform one image to another using backprop and\n",
    "gradient descent? Why does this not always work that well? How is transfer learning\n",
    "used to make it work?"
   ]
  }
 ],
 "metadata": {
  "display_name": "Python 3",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
